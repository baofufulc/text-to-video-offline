import os
import requests
from gtts import gTTS
from moviepy.editor import *
from PIL import Image
from io import BytesIO
import numpy as np

# ---------- é…ç½® ----------
WIDTH, HEIGHT = 1080, 1920
OUTPUT_FILE = "output.mp4"
FONT_PATH = "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf"

ASSETS = {
    "åŠ±å¿—": "https://files.catbox.moe/8m7o6f.mp3",
    "æ‚²ä¼¤": "https://files.catbox.moe/ahgj1r.mp3",
    "æ¸©æŸ”": "https://files.catbox.moe/vtk7oi.mp3",
    "å­¤ç‹¬": "https://files.catbox.moe/xz5s2m.mp3",
    "å¸Œæœ›": "https://files.catbox.moe/1p4lhm.mp3",
}

# ---------- æƒ…ç»ªè¯†åˆ« ----------
def detect_emotion(text):
    if any(w in text for w in ["åŠªåŠ›", "å¥‹æ–—", "æ‹¼", "æ¢¦æƒ³", "åšæŒ"]):
        return "åŠ±å¿—"
    elif any(w in text for w in ["ç—›", "æ³ª", "å¿ƒç¢", "æ‚²ä¼¤", "å¤±æœ›"]):
        return "æ‚²ä¼¤"
    elif any(w in text for w in ["æ¸©æŸ”", "å–œæ¬¢", "ç”œ", "å®‰é™"]):
        return "æ¸©æŸ”"
    elif any(w in text for w in ["å¤œ", "å­¤å•", "å¯‚å¯", "é»‘æš—"]):
        return "å­¤ç‹¬"
    else:
        return "å¸Œæœ›"

# ---------- ç”Ÿæˆ AI èƒŒæ™¯ ----------
def generate_ai_background(text, emotion):
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆ AI èƒŒæ™¯å›¾...")
    prompt = f"ä¸€å¼ {emotion}æ°›å›´çš„å¤œæ™¯ç…§ç‰‡ï¼Œä¸»é¢˜ï¼š{text}ï¼Œç«–å±é«˜æ¸…é£æ ¼"
    url = f"https://image.pollinations.ai/prompt/{prompt}"
    response = requests.get(url)
    img = Image.open(BytesIO(response.content)).resize((WIDTH, HEIGHT))
    img.save("background.jpg")
    print("âœ… èƒŒæ™¯å›¾å·²ç”Ÿæˆ")

# ---------- ä¸­æ–‡è¯­éŸ³ ----------
def create_voice(text):
    print("ğŸ™ï¸ æ­£åœ¨ç”Ÿæˆè¯­éŸ³...")
    tts = gTTS(text, lang="zh-cn")
    tts.save("voice.mp3")

# ---------- å­—å¹•ç”Ÿæˆ ----------
def create_dynamic_subtitles(text, total_duration):
    words = list(text)
    clips = []
    per_word = total_duration / max(len(words), 1)

    for i, w in enumerate(words):
        color = f"rgb({np.random.randint(180,255)}, {np.random.randint(180,255)}, 255)"
        txt_clip = TextClip(
            w,
            fontsize=95,
            color=color,
            font=FONT_PATH,
            method='caption',
            size=(WIDTH - 200, None),
            align='center'
        ).set_position(('center', HEIGHT/2 - 150)).set_duration(per_word)
        txt_clip = txt_clip.crossfadein(0.4).crossfadeout(0.4)
        clips.append(txt_clip.set_start(i * per_word))
    return clips

# ---------- èƒŒæ™¯åŠ¨æ€å…‰å½± ----------
def flicker_effect(get_frame, t):
    frame = get_frame(t).astype(np.float32)
    intensity = 1 + 0.02 * np.sin(2 * np.pi * t)
    frame = np.clip(frame * intensity, 0, 255)
    return frame.astype('uint8')

# ---------- ä¸»ç¨‹åº ----------
def main():
    text = input("è¯·è¾“å…¥è§†é¢‘æ–‡æœ¬ï¼š")
    emotion = detect_emotion(text)
    print(f"ğŸ­ æ£€æµ‹åˆ°æƒ…ç»ªï¼š{emotion}")

    # æ­¥éª¤æ‰§è¡Œ
    create_voice(text)
    generate_ai_background(text, emotion)

    voice = AudioFileClip("voice.mp3")
    duration = voice.duration

    # èƒŒæ™¯è§†é¢‘åŠ åŠ¨æ€æ•ˆæœ
    bg = ImageClip("background.jpg").set_duration(duration).fl(flicker_effect)

    # èƒŒæ™¯éŸ³ä¹ + è¯­éŸ³
    music = AudioFileClip(ASSETS[emotion]).volumex(0.25)
    dynamic_music = music.audio_fadein(1).audio_fadeout(1)
    final_audio = CompositeAudioClip([dynamic_music, voice.volumex(1.3)]).set_duration(duration)

    # å­—å¹•
    subtitles = create_dynamic_subtitles(text, duration)

    # åˆæˆ
    final = CompositeVideoClip([bg, *subtitles]).set_audio(final_audio).resize((WIDTH, HEIGHT))

    print("ğŸ¬ æ­£åœ¨æ¸²æŸ“è§†é¢‘...")
    final.write_videofile(OUTPUT_FILE, fps=24, codec='libx264', audio_codec='aac')
    print("âœ… è§†é¢‘ç”Ÿæˆå®Œæˆï¼šoutput.mp4")

if __name__ == "__main__":
    main()
